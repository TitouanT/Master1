{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titouan Teyssier - TP Classifieur Baysien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "# recuperation des donnée\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# print(iris.data)\n",
    "# print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Division de l'échantillon d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Ciris = np.c_[iris.data.reshape(len(iris.data), -1), iris.target.reshape(len(iris.target), -1)]\n",
    "\n",
    "np.random.seed(100666001)\n",
    "np.random.shuffle(Ciris)\n",
    "shuffledIrisData = Ciris[:, :iris.data.size//len(iris.data)].reshape(iris.data.shape)\n",
    "shuffledIrisTarget = Ciris[:, iris.data.size//len(iris.data):].reshape(iris.target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(shuffledIrisTarget)\n",
    "# print(shuffledIrisData)\n",
    "# print(Ciris)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On divise les données en trois parties:\n",
    "+ Apprentissage 100 premiers example.\n",
    "+ Dev 30 suivants.\n",
    "+ Test 20 derniers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ApprData = shuffledIrisData[:100]\n",
    "class Dataset:\n",
    "    @classmethod\n",
    "    def create(cls, debut, fin):\n",
    "        return cls(shuffledIrisData[debut:fin], shuffledIrisTarget[debut:fin])\n",
    "    def __init__(self, data, target):\n",
    "        self._data = data\n",
    "        self._target = [int(t) for t in target]\n",
    "        self._size = len(data)\n",
    "        self._classes = None\n",
    "        \n",
    "        \n",
    "    def size(self):\n",
    "        return self._size\n",
    "    \n",
    "    def data(self):\n",
    "        return self._data\n",
    "    \n",
    "    def target(self):\n",
    "        return self._target\n",
    "    \n",
    "    def classes(self):\n",
    "        if (self._classes):\n",
    "            return self._classes\n",
    "        \n",
    "        self._classes = []\n",
    "        for c in self._target:\n",
    "            if c not in self._classes:\n",
    "                self._classes.append(c)\n",
    "        \n",
    "        return self._classes\n",
    "    \n",
    "    \n",
    "\n",
    "Apprentissage = Dataset.create(0,100)\n",
    "Dev = Dataset.create(100,130)\n",
    "Test = Dataset.create(130,150)\n",
    "\n",
    "Apprentissage.classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phase d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# class Gaussienne:\n",
    "#     def __init__(self, data):\n",
    "#         self.mu = sum(data)/len(data)\n",
    "#         self.sigma = sum( ((xi - self.mu)**2 for xi in data))/len(data)\n",
    "#         self.const = 1/math.sqrt(2*math.pi*self.sigma)\n",
    "    \n",
    "#     def show(self):\n",
    "#         span = 40 * self.sigma\n",
    "#         x, y = self.calc(self.mu - span, self.mu + span)\n",
    "#         plt.plot(x, y)\n",
    "        \n",
    "#     def calc(self, mini, maxi, n=500):\n",
    "#         dx = (maxi-mini)/n\n",
    "#         x = [mini+i*dx for i in range(n)]\n",
    "#         y = [self.const * math.exp( -((xi-self.mu)**2 / (2*self.sigma)) ) for xi in x]\n",
    "#         return x, y\n",
    "#     def p(self, x):\n",
    "#         return self.const * math.exp( -((x-self.mu)**2 / (2*self.sigma)))\n",
    "        \n",
    "        \n",
    "\n",
    "# class Categorie:\n",
    "#     def __init__(self, cat, data, lendataset):\n",
    "#         self._priori = len(data) / lendataset\n",
    "#         self.name = iris.target_names[cat]\n",
    "#         self.cat = cat\n",
    "        \n",
    "#         self._data = data\n",
    "#         self._gaussiennes = None\n",
    "    \n",
    "#     def priori(self):\n",
    "#         return self._priori\n",
    "    \n",
    "#     def posteriori(self, x, features):\n",
    "#         return self.priori() * self.prob_cond(x, features)\n",
    "    \n",
    "    \n",
    "#     def gaussiennes(self):\n",
    "#         if (self._gaussiennes):\n",
    "#             return self._gaussiennes\n",
    "        \n",
    "#         self._gaussiennes = []\n",
    "        \n",
    "#         for param in range(len(self._data[0])):\n",
    "#             tab = [d[param] for d in self._data]\n",
    "#             self._gaussiennes.append(Gaussienne(tab))\n",
    "#         return self._gaussiennes\n",
    "    \n",
    "#     def prob_cond(self, x, features=[1,1,1,1]):\n",
    "#         prob = 1\n",
    "#         for g, feature, xi in zip(self.gaussiennes(),features, x):\n",
    "#             if feature:\n",
    "#                 prob *= g.p(xi)\n",
    "#         return prob\n",
    "    \n",
    "#     def showRepartition(self):\n",
    "        \n",
    "#         for g in self.gaussiennes():\n",
    "#             g.show()\n",
    "            \n",
    "#         plt.legend(iris.feature_names)\n",
    "#         plt.xlabel(\"cm\")\n",
    "#         plt.ylabel(\"density\")\n",
    "#         plt.title(\"Features pour la catégorie \" + self.name)\n",
    "#         plt.xlim(0,10)\n",
    "#         plt.show()\n",
    "\n",
    "# class Baysien:\n",
    "#     def __init__(self, dataset):\n",
    "#         self._dataset = dataset\n",
    "#         self._priori = None\n",
    "#         self._categories = None\n",
    "\n",
    "    \n",
    "#     def bycat(self):\n",
    "#         if (not self._categories):\n",
    "#             self._categories = []\n",
    "#             for cls in self._dataset.classes():\n",
    "#                 data = [d for d,t in zip(self._dataset.data(), self._dataset.target()) if t == cls]\n",
    "#                 self._categories.append( Categorie(cls, data, self._dataset.size()))\n",
    "#         return self._categories\n",
    "#     def dataset(self):\n",
    "#         return self._dataset\n",
    "#     def selectFeature(self, dev):\n",
    "#         possibilities = [[int(c) for c in ('{0:0'+str(len(dev.data()[0]))+ 'b}').format(i)] for i in range(16)]\n",
    "#         diagMax = 0\n",
    "#         acceptable = []\n",
    "#         for features in possibilities[1:]:\n",
    "#             confMat = self.confusion(features, dev)\n",
    "#             diag = sum((confMat[i][i] for i in range(len(confMat))))\n",
    "#             if (diag == diagMax):\n",
    "#                 acceptable.append(features)\n",
    "#             elif (diag > diagMax):\n",
    "#                 diagMax = diag\n",
    "#                 acceptable = [features]\n",
    "#         return acceptable\n",
    "        \n",
    "        \n",
    "            \n",
    "#     def confusion(self, features, dev):\n",
    "#         mat = []\n",
    "#         for i in range(len(self.bycat())):\n",
    "#             row = []\n",
    "#             for j in range(len(self.bycat())):\n",
    "#                 row.append(0)\n",
    "#             mat.append(row)\n",
    "        \n",
    "#         for x,y in zip(dev.data(), dev.target()):\n",
    "#             # calcul des probabilité a posteriori\n",
    "#             p, guess = max([(cat.posteriori(x, features), cat) for cat in self.bycat()])\n",
    "#             mat[guess.cat][y] += 1\n",
    "        \n",
    "#         return mat\n",
    "            \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app = Baysien(Apprentissage)\n",
    "# for cat in app.bycat():\n",
    "#     cat.showRepartition()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for dataset in [Apprentissage, Dev, Test]:\n",
    "#     feat = app.selectFeature(dataset)\n",
    "#     for f in feat:\n",
    "#         print(f, app.confusion(f, Test))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [g.mu for g in app.bycat()[0].gaussiennes()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loi Gaussienne MultiDimensionel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMat(m):\n",
    "    for r in m:\n",
    "        print(r)\n",
    "    print()\n",
    "\n",
    "# fonction pour manipuler des matrice:\n",
    "def determinant(mat):\n",
    "    n=len(mat)\n",
    "    if (n == 1):\n",
    "        return mat[0][0]\n",
    "    sign = 1\n",
    "    det = 0\n",
    "    for i in range(n):\n",
    "        det += sign * mat[i][0] * determinant([l[1:] for l in mat[:i]+mat[i+1:]])\n",
    "        sign *= -1\n",
    "    return det\n",
    "\n",
    "# a besoin du determinant\n",
    "def comatrice(mat):\n",
    "    com = []\n",
    "    lsign = 1\n",
    "    n = len(mat)\n",
    "    for i in range(n):\n",
    "        sign = lsign\n",
    "        line = []\n",
    "        com.append(line)\n",
    "        for j in range(n):\n",
    "            cofactor = sign * determinant([l[:j]+l[j+1:] for l in mat[:i]+mat[i+1:]])\n",
    "            sign*=-1\n",
    "            line.append(cofactor)\n",
    "        lsign*=-1\n",
    "    return com\n",
    "\n",
    "def transpose(mat):\n",
    "    tmat=[]\n",
    "    for j in range(len(mat[0])):\n",
    "        line = []\n",
    "        for i in range(len(mat)):\n",
    "            line.append(mat[i][j])\n",
    "        tmat.append(line)\n",
    "    return tmat\n",
    "\n",
    "# a besoin du determinant, de transpose et de comatrice\n",
    "def inverse(mat):\n",
    "    d = determinant(mat)\n",
    "    if d == 0:\n",
    "        return None\n",
    "    inv = transpose(comatrice(mat))\n",
    "    for l in inv:\n",
    "        for i in range(len(l)):\n",
    "            l[i] /= d\n",
    "    return inv\n",
    "\n",
    "# a besoin de transpose pour simplifier\n",
    "def produit(a, b):\n",
    "    if (len(a[0]) != len(b)):\n",
    "        return None\n",
    "    prod = []\n",
    "    tb = transpose(b)\n",
    "    for l in a:\n",
    "        line = []\n",
    "        prod.append(line)\n",
    "        for c in tb:\n",
    "            line.append(sum((va*vb for va,vb in zip(l,c)) ))\n",
    "    \n",
    "    return prod\n",
    "\n",
    "\n",
    "\n",
    "# supprime la ligne L et la colonne C\n",
    "def supprimeLC(mat, L, C):\n",
    "    return [l[:C]+l[C+1:] for l in mat[:L]+mat[L+1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application d'un masque de caractéristique\n",
    "def transform_sigma(sigma, mask):\n",
    "    for i, v in reversed(list(enumerate(mask))):\n",
    "        if not v:\n",
    "            sigma = supprimeLC(sigma, i, i)\n",
    "    return sigma\n",
    "\n",
    "def transform_individu(ind, mask):\n",
    "    ind=list(ind)\n",
    "    for i, v in reversed(list(enumerate(mask))):\n",
    "        if not v:\n",
    "            ind = ind[:i]+ind[i+1:]\n",
    "    return ind\n",
    "\n",
    "def transform_params(sigma, mu, mask):\n",
    "    return (transform_sigma(sigma, mask), transform_individu(mu, mask))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de la vraisemblance\n",
    "import math\n",
    "def gaussienneMultiD(x, sigma, mu):\n",
    "    det = determinant(sigma)\n",
    "    dim = len(x)\n",
    "    denominateur = ((2 * math.pi)**dim * det)**0.5\n",
    "    txmu = [[xi-mui for xi,mui in zip(x, mu)]]\n",
    "    xmu = transpose(txmu)\n",
    "    inv = inverse(sigma)\n",
    "    dist2 = produit(produit(txmu, inv), xmu)[0][0]\n",
    "    return math.exp(-dist2/2) / denominateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec des individus donné on peut calculer la matrice de covariance et l'individu moyen\n",
    "def calcul_des_parametres(individus):\n",
    "    n = len(individus)\n",
    "    features = transpose(individus)\n",
    "    mus = [sum(feature)/n for feature in features]\n",
    "    \n",
    "    features_norm = [[v-mu for v in feature] for feature,mu in zip(features,mus)]\n",
    "    \n",
    "    varcovar_almost = produit(features_norm, transpose(features_norm))\n",
    "    sigma = [[v/n for v in line] for line in varcovar_almost]\n",
    "    return (sigma, mus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_generateur_de_classifieur(donner_apprentissage):\n",
    "    # phase d'apprentissage, on apprend pour chaque classe sa probabilité à priori\n",
    "    # ainsi que les parametre nécessaire au calcul de la vraissemblance\n",
    "    \n",
    "    \n",
    "    \n",
    "    individus = donner_apprentissage.data()\n",
    "    cibles = donner_apprentissage.target()\n",
    "    classes = donner_apprentissage.classes()\n",
    "    \n",
    "    individu_par_classe = [[i for i, classe in zip(individus, cibles) if classe == cls] for cls in classes]\n",
    "    apriori_par_classe = [len(i)/len(individus) for i in individu_par_classe]\n",
    "    param_vraisemblance_par_classe = [calcul_des_parametres(individu) for individu in individu_par_classe]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # phase de développement, on influt sur le calcul de la vraisemblance\n",
    "    # en choisissant les critères à observé\n",
    "    def generateur_de_classifieur(features_mask):\n",
    "        \n",
    "        custom_params = [transform_params(sigma, mu, features_mask) for sigma, mu in param_vraisemblance_par_classe]\n",
    "        \n",
    "        #le classifieur calcul les vraisemblance pour chaque classes\n",
    "        #puis calcul les proba a posteriori\n",
    "        #et decide en faveur de la plus haute proba a posteriori\n",
    "        def classifieur_baysien(x):\n",
    "            x = transform_individu(x, features_mask)\n",
    "            vraissemblance_par_classe = [gaussienneMultiD(x,sigma,mu) for sigma,mu in custom_params]\n",
    "            posteriori = [ap* vr for ap,vr in zip(apriori_par_classe, vraissemblance_par_classe)]\n",
    "            \n",
    "            max_posteriori = posteriori[0]\n",
    "            guess = 0\n",
    "            for cls, p in enumerate(posteriori):\n",
    "                if (p > max_posteriori):\n",
    "                    max_posteriori = p\n",
    "                    guess = cls\n",
    "            \n",
    "            return guess\n",
    "            \n",
    "        \n",
    "        return classifieur_baysien\n",
    "    \n",
    "    return generateur_de_classifieur\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation du generateur de classifieur\n",
    "generateur = creer_generateur_de_classifieur(Apprentissage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[False, False, False, True],\n",
       " [False, False, True, False],\n",
       " [False, False, True, True],\n",
       " [False, True, False, False],\n",
       " [False, True, False, True],\n",
       " [False, True, True, False],\n",
       " [False, True, True, True],\n",
       " [True, False, False, False],\n",
       " [True, False, False, True],\n",
       " [True, False, True, False],\n",
       " [True, False, True, True],\n",
       " [True, True, False, False],\n",
       " [True, True, False, True],\n",
       " [True, True, True, False],\n",
       " [True, True, True, True]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creation de tous les masque possible\n",
    "def createMasks(nFeature):\n",
    "    return [[c=='1' for c in ('{0:0'+str(nFeature)+'b}').format(i)] for i in range(1,2**nFeature)]\n",
    "masks = createMasks(4)\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrice de confusion\n",
    "def confusion(classifieur, dataset):\n",
    "    inds = dataset.data()\n",
    "    targ = dataset.target()\n",
    "    conf = [[0 for c in dataset.classes()] for c in dataset.classes()]\n",
    "    for i,t in zip(inds, targ):\n",
    "        conf[classifieur(i)][t] += 1\n",
    "    return conf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction d'evaluation d'un classifieur\n",
    "def evaluate_classifieur(classifieur, dataset):\n",
    "    lines = confusion(classifieur, dataset)\n",
    "    cols = transpose(lines)\n",
    "    nClasse = len(dataset.classes())\n",
    "\n",
    "    instances_classe_i = [sum(line) for line in lines]\n",
    "    instances_réelement_i = [sum(col) for col in cols]\n",
    "    instances_correctement_classe_i = [line[i] for i, line in enumerate(lines)]\n",
    "    \n",
    "    precision_i = [oki/cli for oki, cli in zip(instances_correctement_classe_i, instances_classe_i) if cli > 0]\n",
    "    rappel_i = [oki/rei for oki, rei in zip(instances_correctement_classe_i, instances_réelement_i) if rei > 0]\n",
    "    n = sum(instances_classe_i)\n",
    "    nOK = sum(instances_correctement_classe_i)\n",
    "    nKO = n-nOK\n",
    "    CER = nKO/n\n",
    "    rappel = sum(rappel_i) / nClasse\n",
    "    precision = sum(precision_i) / nClasse\n",
    "    \n",
    "    stats = {}\n",
    "    stats[\"taux_erreur\"] = CER * 100\n",
    "    stats[\"taux_erreur_±\"] = 196 * (CER*(1-CER)/(n**2))**0.5\n",
    "    stats[\"taux_bonne_classification\"] = nOK / n * 100\n",
    "    stats[\"precision\"] = precision * 100\n",
    "    stats[\"rappel\"] = rappel * 100\n",
    "    stats[\"fmesure\"] = 2*rappel*precision/(rappel+precision) * 100\n",
    "    return stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, True]\n",
      "{'taux_erreur': 66.66666666666666, 'taux_erreur_±': 3.0798428691680737, 'taux_bonne_classification': 33.33333333333333, 'precision': 11.11111111111111, 'rappel': 33.33333333333333, 'fmesure': 16.666666666666664}\n",
      "\n",
      "[False, False, True, False]\n",
      "{'taux_erreur': 66.66666666666666, 'taux_erreur_±': 3.0798428691680737, 'taux_bonne_classification': 33.33333333333333, 'precision': 11.11111111111111, 'rappel': 33.33333333333333, 'fmesure': 16.666666666666664}\n",
      "\n",
      "[False, False, True, True]\n",
      "{'taux_erreur': 3.3333333333333335, 'taux_erreur_±': 1.1727692246648476, 'taux_bonne_classification': 96.66666666666667, 'precision': 96.29629629629629, 'rappel': 97.22222222222221, 'fmesure': 96.75704412546517}\n",
      "\n",
      "[False, True, False, False]\n",
      "{'taux_erreur': 73.33333333333333, 'taux_erreur_±': 2.8891487062651486, 'taux_bonne_classification': 26.666666666666668, 'precision': 8.88888888888889, 'rappel': 33.33333333333333, 'fmesure': 14.035087719298245}\n",
      "\n",
      "[False, True, False, True]\n",
      "{'taux_erreur': 6.666666666666667, 'taux_erreur_±': 1.6296996617948722, 'taux_bonne_classification': 93.33333333333333, 'precision': 93.33333333333333, 'rappel': 94.44444444444446, 'fmesure': 93.88560157790926}\n",
      "\n",
      "[False, True, True, False]\n",
      "{'taux_erreur': 13.333333333333334, 'taux_erreur_±': 2.220906277031524, 'taux_bonne_classification': 86.66666666666667, 'precision': 86.66666666666666, 'rappel': 87.5, 'fmesure': 87.08133971291866}\n",
      "\n",
      "[False, True, True, True]\n",
      "{'taux_erreur': 6.666666666666667, 'taux_erreur_±': 1.6296996617948722, 'taux_bonne_classification': 93.33333333333333, 'precision': 93.33333333333333, 'rappel': 94.44444444444446, 'fmesure': 93.88560157790926}\n",
      "\n",
      "[True, False, False, False]\n",
      "{'taux_erreur': 66.66666666666666, 'taux_erreur_±': 3.0798428691680737, 'taux_bonne_classification': 33.33333333333333, 'precision': 11.11111111111111, 'rappel': 33.33333333333333, 'fmesure': 16.666666666666664}\n",
      "\n",
      "[True, False, False, True]\n",
      "{'taux_erreur': 0.0, 'taux_erreur_±': 0.0, 'taux_bonne_classification': 100.0, 'precision': 100.0, 'rappel': 100.0, 'fmesure': 100.0}\n",
      "\n",
      "[True, False, True, False]\n",
      "{'taux_erreur': 10.0, 'taux_erreur_±': 1.96, 'taux_bonne_classification': 90.0, 'precision': 89.56228956228956, 'rappel': 90.27777777777779, 'fmesure': 89.91861039602674}\n",
      "\n",
      "[True, False, True, True]\n",
      "{'taux_erreur': 3.3333333333333335, 'taux_erreur_±': 1.1727692246648476, 'taux_bonne_classification': 96.66666666666667, 'precision': 96.29629629629629, 'rappel': 97.22222222222221, 'fmesure': 96.75704412546517}\n",
      "\n",
      "[True, True, False, False]\n",
      "{'taux_erreur': 23.333333333333332, 'taux_erreur_±': 2.7632902199201177, 'taux_bonne_classification': 76.66666666666667, 'precision': 76.0942760942761, 'rappel': 76.38888888888889, 'fmesure': 76.24129788082314}\n",
      "\n",
      "[True, True, False, True]\n",
      "{'taux_erreur': 6.666666666666667, 'taux_erreur_±': 1.6296996617948722, 'taux_bonne_classification': 93.33333333333333, 'precision': 93.33333333333333, 'rappel': 94.44444444444446, 'fmesure': 93.88560157790926}\n",
      "\n",
      "[True, True, True, False]\n",
      "{'taux_erreur': 10.0, 'taux_erreur_±': 1.96, 'taux_bonne_classification': 90.0, 'precision': 90.90909090909092, 'rappel': 91.66666666666666, 'fmesure': 91.28630705394191}\n",
      "\n",
      "[True, True, True, True]\n",
      "{'taux_erreur': 6.666666666666667, 'taux_erreur_±': 1.6296996617948722, 'taux_bonne_classification': 93.33333333333333, 'precision': 93.33333333333333, 'rappel': 94.44444444444446, 'fmesure': 93.88560157790926}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creation de tous les classifieur possible et evaluation\n",
    "classifieurs = [generateur(mask) for mask in masks]\n",
    "\n",
    "dev_evaluations = [evaluate_classifieur(c, Dev) for c in classifieurs]\n",
    "for ev, mask in zip(dev_evaluations, masks):\n",
    "    print(mask)\n",
    "    print(ev)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction réutilisable qui rend le meilleur mask selon le dataset fournit\n",
    "def get_best_mask(generateur, development_dataset):\n",
    "    masks = createMasks(4);\n",
    "    bestmask = None\n",
    "    lowest_error_rate = 200\n",
    "    for mask in masks:\n",
    "        classifieur = generateur(mask)\n",
    "        stats = evaluate_classifieur(classifieur, development_dataset)\n",
    "        err = stats[\"taux_erreur\"]\n",
    "        if (err < lowest_error_rate):\n",
    "            lowest_error_rate = err\n",
    "            bestmask = mask\n",
    "    \n",
    "    return bestmask\n",
    "        \n",
    "\n",
    "bestmask = get_best_mask(generateur, Dev)\n",
    "best_model = generateur(bestmask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistique de prediction (stats sur apprentissage ):\n",
      "De cette matrice de confusion:\n",
      "[34, 0, 0]\n",
      "[0, 31, 2]\n",
      "[0, 1, 32]\n",
      "\n",
      "On obtient les statistiques suivantes:\n",
      "{'taux_erreur': 3.0, 'taux_erreur_±': 0.3343509533409468, 'taux_bonne_classification': 97.0, 'precision': 96.96969696969697, 'rappel': 96.99754901960786, 'fmesure': 96.98362099499344}\n",
      "\n",
      "Statistique de generalisation (stats sur test ):\n",
      "De cette matrice de confusion:\n",
      "[5, 0, 0]\n",
      "[1, 5, 1]\n",
      "[0, 1, 7]\n",
      "\n",
      "On obtient les statistiques suivantes:\n",
      "{'taux_erreur': 15.0, 'taux_erreur_±': 3.499299929985997, 'taux_bonne_classification': 85.0, 'precision': 86.30952380952381, 'rappel': 84.72222222222223, 'fmesure': 85.50850734725445}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def presente_stats(classifieur, data, description, nom):\n",
    "        \n",
    "        stats = evaluate_classifieur(classifieur, data)\n",
    "        conf = confusion(classifieur, data)\n",
    "        print(\"Statistique de\",description,\"(stats sur\",nom,\"):\")\n",
    "        print(\"De cette matrice de confusion:\")\n",
    "        printMat(conf)\n",
    "        print(\"On obtient les statistiques suivantes:\")\n",
    "        print(stats)\n",
    "        print()\n",
    "\n",
    "def evaluate_prediction_generalisation(classifieur, data_app, data_test):\n",
    "    \n",
    "    \n",
    "    \n",
    "    presente_stats(classifieur, data_app, \"prediction\", \"apprentissage\")\n",
    "    presente_stats(classifieur, data_test, \"generalisation\", \"test\")\n",
    "\n",
    "evaluate_prediction_generalisation(best_model, Apprentissage, Test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_classification(model, dataset, name):\n",
    "    classification = [(list(x), model(x)) for x in dataset.data()]\n",
    "    print('{:#^50}'.format(name))\n",
    "    for l in classification:\n",
    "        print('#',l)\n",
    "    print('{:#^50}'.format(''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_classification(best_model, Dev, \"Dev\")\n",
    "# dump_classification(best_model, Test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################Dev########################\n",
    "# ([5.5, 2.4, 3.7, 1.0], 1)\n",
    "# ([6.5, 2.8, 4.6, 1.5], 1)\n",
    "# ([7.6, 3.0, 6.6, 2.1], 2)\n",
    "# ([5.4, 3.9, 1.3, 0.4], 0)\n",
    "# ([5.0, 3.3, 1.4, 0.2], 0)\n",
    "# ([4.8, 3.0, 1.4, 0.3], 0)\n",
    "# ([4.8, 3.1, 1.6, 0.2], 0)\n",
    "# ([6.2, 3.4, 5.4, 2.3], 2)\n",
    "# ([6.3, 2.7, 4.9, 1.8], 2)\n",
    "# ([4.6, 3.4, 1.4, 0.3], 0)\n",
    "# ([7.3, 2.9, 6.3, 1.8], 2)\n",
    "# ([6.0, 2.7, 5.1, 1.6], 1)\n",
    "# ([5.9, 3.0, 5.1, 1.8], 2)\n",
    "# ([5.5, 2.5, 4.0, 1.3], 1)\n",
    "# ([5.1, 3.8, 1.9, 0.4], 0)\n",
    "# ([5.6, 2.7, 4.2, 1.3], 1)\n",
    "# ([6.7, 3.3, 5.7, 2.1], 2)\n",
    "# ([6.3, 2.5, 4.9, 1.5], 1)\n",
    "# ([5.4, 3.0, 4.5, 1.5], 1)\n",
    "# ([5.5, 2.4, 3.8, 1.1], 1)\n",
    "# ([5.1, 3.3, 1.7, 0.5], 0)\n",
    "# ([5.8, 2.7, 3.9, 1.2], 1)\n",
    "# ([6.1, 3.0, 4.9, 1.8], 2)\n",
    "# ([5.0, 3.4, 1.5, 0.2], 0)\n",
    "# ([5.6, 2.9, 3.6, 1.3], 1)\n",
    "# ([6.6, 2.9, 4.6, 1.3], 1)\n",
    "# ([6.8, 2.8, 4.8, 1.4], 1)\n",
    "# ([5.4, 3.7, 1.5, 0.2], 0)\n",
    "# ([5.0, 3.0, 1.6, 0.2], 0)\n",
    "# ([7.7, 3.0, 6.1, 2.3], 2)\n",
    "##################################################\n",
    "\n",
    "#######################Test#######################\n",
    "# ([6.1, 2.8, 4.0, 1.3], 1)\n",
    "# ([6.9, 3.1, 4.9, 1.5], 1)\n",
    "# ([5.1, 3.7, 1.5, 0.4], 0)\n",
    "# ([5.0, 3.5, 1.6, 0.6], 1)\n",
    "# ([6.1, 3.0, 4.6, 1.4], 1)\n",
    "# ([6.1, 2.9, 4.7, 1.4], 1)\n",
    "# ([6.7, 3.0, 5.2, 2.3], 2)\n",
    "# ([6.9, 3.2, 5.7, 2.3], 2)\n",
    "# ([4.9, 3.6, 1.4, 0.1], 0)\n",
    "# ([6.4, 2.8, 5.6, 2.2], 2)\n",
    "# ([4.4, 2.9, 1.4, 0.2], 0)\n",
    "# ([6.6, 3.0, 4.4, 1.4], 1)\n",
    "# ([6.0, 3.0, 4.8, 1.8], 2)\n",
    "# ([5.9, 3.2, 4.8, 1.8], 2)\n",
    "# ([7.7, 2.6, 6.9, 2.3], 2)\n",
    "# ([5.5, 4.2, 1.4, 0.2], 0)\n",
    "# ([6.3, 3.3, 6.0, 2.5], 2)\n",
    "# ([6.1, 2.6, 5.6, 1.4], 1)\n",
    "# ([5.1, 3.5, 1.4, 0.2], 0)\n",
    "# ([5.8, 2.8, 5.1, 2.4], 2)\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Peu maintenant terminer par classer tout le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistique de production (stats sur tout le corpus ):\n",
      "De cette matrice de confusion:\n",
      "[49, 0, 0]\n",
      "[1, 48, 3]\n",
      "[0, 2, 47]\n",
      "\n",
      "On obtient les statistiques suivantes:\n",
      "{'taux_erreur': 4.0, 'taux_erreur_±': 0.25605332777893486, 'taux_bonne_classification': 96.0, 'precision': 96.07535321821035, 'rappel': 96.0, 'fmesure': 96.0376618281679}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# on fini par faire réapprendre le modele sur tout le corpus\n",
    "Corpus = Dataset.create(0,150)\n",
    "presente_stats(best_model, Corpus, \"production\", \"tout le corpus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matrice de confusion de mon meilleur modele:\n",
    "[49, 0, 0]\n",
    "\n",
    "[**1**, 48, **3**]  Classement de 3 Virginica en Versicolor et de une Setosa en Versicolor\n",
    "\n",
    "[0, **2**, 47]  Classement de 2 Versicolor en Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "\n",
    "# On obtient les statistiques suivantes:\n",
    "# {'taux_erreur': 3.3333333333333335, 'taux_erreur_±': 0.2345538449329695, 'taux_bonne_classification': 96.66666666666667, 'precision': 96.67867146858742, 'rappel': 96.66666666666667, 'fmesure': 96.6726686949383}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Par curiosité, on peut aussi regarder sur notre modele final quel masque aurait les meilleurs performance\n",
    "(en prediction !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistique de Triche... (stats sur tout le corpus ):\n",
      "De cette matrice de confusion:\n",
      "[50, 0, 0]\n",
      "[0, 49, 2]\n",
      "[0, 1, 48]\n",
      "\n",
      "On obtient les statistiques suivantes:\n",
      "{'taux_erreur': 2.0, 'taux_erreur_±': 0.18293333333333334, 'taux_bonne_classification': 98.0, 'precision': 98.01253834867282, 'rappel': 98.0, 'fmesure': 98.00626877331567}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gene = creer_generateur_de_classifieur(Corpus)\n",
    "curiosity_bestmask = get_best_mask(gene, Corpus)\n",
    "alter_ego = gene(curiosity_bestmask)\n",
    "presente_stats(alter_ego, Corpus, \"Triche...\", \"tout le corpus\") # -40% d'erreur ! (-2 erreur)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
